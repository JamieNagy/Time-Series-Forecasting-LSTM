{
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "name": "Torch_MultiRegression_GlobalLSTM_StoreEmbedding"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 29781,
          "databundleVersionId": 2887556,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'store-sales-time-series-forecasting:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F29781%2F2887556%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240915%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240915T033149Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dac6c493ccdfadc939b1888952d628b1128204c060111f260842c985bf3847e32ddc2dbd92e2f4a998c6d9f209f403bb0c4647631260af962540124b613ede25b330436b3f5031febe5bc8d29c099353bff748c304deb8544fb53fbab6211bd4b1e733e6dc3c897db01e5135679657da77f79884609268ce639899a3c7e4c53db33da611a8311c16c019611ce2b5691f9369675230add6e2b757e5a2a9b2404368683b36800b00daff83ec52ed237522f488db1ab0392bf3ebb7fbab73c435b1e9644fc91652708c3f75576ea365f767dede447a70f137448039f7f752e69dbb6fb34fbe9f5e55b0c05cc9c3ac3c18f46c208344ce05dbcae2404a5ecb334eef4'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "LhgHf8jBuZaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "UsaSCznj_COO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import scipy.stats as ss\n",
        "! pip install sktime pmdarima pytorch-lightning\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings('ignore', message='Level of selected dummy variable  lower level than base ts_frequency.')\n",
        "\n",
        "SEED = 42\n",
        "KAGGLE_INPUT_PATH= '/kaggle/input/store-sales-time-series-forecasting'\n",
        "KAGGLE_WORKING_PATH = '/kaggle/working'"
      ],
      "metadata": {
        "id": "4emABXO8lHZb",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:40:14.145591Z",
          "iopub.execute_input": "2024-07-03T20:40:14.145954Z",
          "iopub.status.idle": "2024-07-03T20:40:33.483953Z",
          "shell.execute_reply.started": "2024-07-03T20:40:14.145924Z",
          "shell.execute_reply": "2024-07-03T20:40:33.483046Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtypes = {\n",
        "    'sales': float,\n",
        "    # train and test\n",
        "    'date': str,\n",
        "    'store_nbr': int,\n",
        "    'family': 'category',\n",
        "    'onpromotion': int,\n",
        "    # oil - date\n",
        "    'dcoilwtico': float,\n",
        "    # holiday - date\n",
        "    'type': 'category',\n",
        "    'locale': 'category',\n",
        "    'locale_name': 'string',\n",
        "    'description': 'string',\n",
        "    'transferred': bool,\n",
        "    # store - store_nbr\n",
        "    'city': 'category',\n",
        "    'state': 'category',\n",
        "    'cluster': 'category',\n",
        "    # transactions - date and store_nbr\n",
        "    'transactions': int\n",
        "}\n",
        "\n",
        "df_train = pd.read_csv(os.path.join(KAGGLE_INPUT_PATH, 'train.csv'),\n",
        "                       dtype=dtypes)\n",
        "families = list(df_train.family.cat.categories)\n",
        "print(families)\n",
        "stores = list(df_train.store_nbr.unique())\n",
        "print(stores)\n",
        "df_test = pd.read_csv(os.path.join(KAGGLE_INPUT_PATH, 'test.csv'),\n",
        "                      dtype=dtypes)\n",
        "print(f'Found {len(df_train)} rows for training and {len(df_test)} rows for testing')\n",
        "df_oil = pd.read_csv(os.path.join(KAGGLE_INPUT_PATH, 'oil.csv'),\n",
        "                       dtype=dtypes)\n",
        "df_oil.rename(columns={'dcoilwtico': 'oil_price'}, inplace=True)\n",
        "df_holy = pd.read_csv(os.path.join(KAGGLE_INPUT_PATH, 'holidays_events.csv'),\n",
        "                       dtype=dtypes)\n",
        "df_holy.rename(columns={'type': 'holiday_type'}, inplace=True)\n",
        "df_store = pd.read_csv(os.path.join(KAGGLE_INPUT_PATH, 'stores.csv'),\n",
        "                       dtype=dtypes)\n",
        "df_store.rename(columns={'type': 'store_type'}, inplace=True)\n",
        "df_trans = pd.read_csv(os.path.join(KAGGLE_INPUT_PATH, 'transactions.csv'),\n",
        "                       dtype=dtypes)\n",
        "df_sample = pd.read_csv(os.path.join(KAGGLE_INPUT_PATH, 'sample_submission.csv'))\n",
        "df_sample.head()\n",
        "\n",
        "# transform store_nbr as categorical starting from zero\n",
        "for df in [df_train, df_test, df_store, df_trans]:\n",
        "  df['store_nbr'] = pd.Categorical(df['store_nbr'] - 1)\n",
        "stores = sorted(list(df_train.store_nbr.cat.categories))\n",
        "\n",
        "# transform date to datetime\n",
        "for df in [df_train, df_test, df_oil, df_holy, df_trans]:\n",
        "  df['date'] = pd.to_datetime(df['date'])"
      ],
      "metadata": {
        "id": "gPSfdHXInCQc",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:40:41.798554Z",
          "iopub.execute_input": "2024-07-03T20:40:41.79957Z",
          "iopub.status.idle": "2024-07-03T20:40:45.193238Z",
          "shell.execute_reply.started": "2024-07-03T20:40:41.799534Z",
          "shell.execute_reply": "2024-07-03T20:40:45.19247Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Sales and Onpromotion Data"
      ],
      "metadata": {
        "id": "DjuiB8D0KLjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def sort_df(df):\n",
        "  return df.sort_values(by=['date', 'store_nbr'])\n",
        "\n",
        "def pivot_family_columns(df):\n",
        "  index = ['date', 'store_nbr']\n",
        "  onpromotion = df.pivot_table(index=index, columns='family', values='onpromotion',\n",
        "                                   aggfunc='sum', sort=False).reset_index()\n",
        "  if 'sales' in df.columns:\n",
        "    col_pred = 'sales'\n",
        "  else:\n",
        "    df = df.reset_index()\n",
        "    col_pred = 'id'\n",
        "  pred = df.pivot_table(index=index, columns='family', values=col_pred,\n",
        "                             aggfunc='sum', sort=False).reset_index()\n",
        "  df = pd.merge(onpromotion, pred, on=index, suffixes=('_onpromotion', f'_{col_pred}'))\n",
        "  df.columns.name = None\n",
        "  return sort_df(df)\n",
        "\n",
        "def get_sum_of_scaled_sales(df_sales):\n",
        "  scaler = MinMaxScaler()\n",
        "  # Apply MinMax scaling to each sales column before adding them up\n",
        "  scaled_sales = scaler.fit_transform(df_sales)\n",
        "  return scaled_sales.sum(axis=1)\n",
        "\n",
        "# Define the final df_train reference\n",
        "# from this point on the Notebook is immutable on df_train and df_test\n",
        "df_train = pivot_family_columns(df_train)\n",
        "sales_columns = [col for col in df_train.columns if col.endswith('_sales')]\n",
        "df_train['sales_MM'] = get_sum_of_scaled_sales(df_train[sales_columns])\n",
        "df_test = pivot_family_columns(df_test)\n",
        "id_columns = [col for col in list(df_test.columns) if col.endswith('_id')]\n",
        "onpromotion_columns = [col for col in df_train.columns if col.endswith('_onpromotion')]"
      ],
      "metadata": {
        "id": "I_lIQ_yTdWWR",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:40:52.350814Z",
          "iopub.execute_input": "2024-07-03T20:40:52.351201Z",
          "iopub.status.idle": "2024-07-03T20:40:54.532984Z",
          "shell.execute_reply.started": "2024-07-03T20:40:52.351169Z",
          "shell.execute_reply": "2024-07-03T20:40:54.531832Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline: Preprocess Onpromotion and Sales Per Store and Product Family"
      ],
      "metadata": {
        "id": "5K9evYegKT33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pmdarima.utils import diff, diff_inv\n",
        "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
        "\n",
        "class StorewiseTransformer:\n",
        "  # assumes that incoming data is sorted as date, store_nbr\n",
        "  # returns in order date, store_nbr\n",
        "  def __init__(self, product_families: list[str],\n",
        "               diff_period: int, lag_period: int,\n",
        "               apply_power: bool, apply_scaling: bool):\n",
        "      self.product_families = product_families\n",
        "      self.groupby_column = 'store_nbr'\n",
        "      if apply_power:\n",
        "        self.power_transformers = {}\n",
        "      else:\n",
        "        self.power_transformers = None\n",
        "      if apply_scaling:\n",
        "        self.scalers = {}\n",
        "      else:\n",
        "        self.scalers = None\n",
        "      self.diff_period = diff_period\n",
        "      self.apply_diff = diff_period > 0\n",
        "      if self.apply_diff:\n",
        "        self.diff_last_known = {}\n",
        "        self.train_shifted = {}\n",
        "      else:\n",
        "        self.diff_last_known = None\n",
        "        self.train_shifted = None\n",
        "      self.lag_period = lag_period\n",
        "      self.derive_lags = lag_period > 0\n",
        "      if self.derive_lags:\n",
        "        self.lag_last_known = {}\n",
        "        self.feat_lag = [f'{col}_{lag_period}' for col in product_families]\n",
        "      else:\n",
        "        self.lag_last_known = None\n",
        "        self.feat_lag = []\n",
        "\n",
        "  def _combine_group_data(self, df_list):\n",
        "      return pd.concat(df_list).sort_values(\n",
        "          by=['date', self.groupby_column])\n",
        "\n",
        "  def _fit_transform_power(self, store, group):\n",
        "    power_transformers = {}\n",
        "    self.power_transformers[store] = power_transformers\n",
        "    for family in self.product_families:\n",
        "      sales = group[family].values\n",
        "      if np.all(sales == sales[0]):\n",
        "        transformer = sales[0]\n",
        "        sales = 0.0\n",
        "      else:\n",
        "        transformer = PowerTransformer(standardize=False)\n",
        "        sales = sales.reshape(-1, 1)\n",
        "        transformer.fit(sales)\n",
        "        lmbda = transformer.lambdas_[0]\n",
        "        if lmbda < 1e-2:\n",
        "          transformer.lambdas_[0] = 0\n",
        "        sales = transformer.transform(sales)\n",
        "      power_transformers[family] = transformer\n",
        "      group[family] = sales\n",
        "\n",
        "  def _transform_power(self, store, group):\n",
        "    power_transformers = self.power_transformers[store]\n",
        "    for family in self.product_families:\n",
        "      transformer = power_transformers[family]\n",
        "      if isinstance(transformer, (np.int64, float)):\n",
        "        group[family] = 0.0\n",
        "      else:\n",
        "        group[family] = transformer.transform(\n",
        "            group[family].values.reshape(-1, 1))\n",
        "\n",
        "  def _inverse_transform_power(self, store, group):\n",
        "    power_transformers = self.power_transformers[store]\n",
        "    for family in self.product_families:\n",
        "      transformer = power_transformers[family]\n",
        "      if isinstance(transformer, (np.int64, float)):\n",
        "        sales = transformer\n",
        "      else:\n",
        "        sales = transformer.inverse_transform(\n",
        "            group[family].values.reshape(-1, 1))\n",
        "      group[family] = sales\n",
        "\n",
        "  def _fit_transform_differencing(self, store, group):\n",
        "    families, period = self.product_families, self.diff_period\n",
        "    group_shifted = group.copy()\n",
        "    group_shifted[families] = group[families].shift(periods=period)\n",
        "    self.train_shifted[store] = group_shifted\n",
        "    self.diff_last_known[store] = group[families].iloc[-period:].copy()\n",
        "    group.iloc[-period:, group.columns.get_loc(\n",
        "        'is_used_for_diff_lag')] = True\n",
        "    padded_ts = np.vstack((np.zeros((period, len(families))),\n",
        "                                group[families].values))\n",
        "    for ts, family in zip(padded_ts.T, families):\n",
        "      group[family] = diff(ts, lag=period)\n",
        "\n",
        "  def _transform_differencing_unseen(self, store, group):\n",
        "    last_and_new = np.vstack((self.diff_last_known[store].values,\n",
        "                              group[self.product_families].values))\n",
        "    for ts, family in zip(last_and_new.T, self.product_families):\n",
        "      group[family] = diff(ts, lag=self.diff_period)\n",
        "\n",
        "  def _inverse_transform_differencing_training(self, store, group):\n",
        "    group_shifted = self.train_shifted[store]\n",
        "    group_shifted = group_shifted[group_shifted.date.isin(group.date)]\n",
        "    group[self.product_families] = group[self.product_families].values + group_shifted[self.product_families].values\n",
        "\n",
        "  def _inverse_transform_differencing_unseen(self, store, group):\n",
        "    last_and_new = np.vstack((self.diff_last_known[store].values,\n",
        "                              group[self.product_families].values))\n",
        "    for ts, family in zip(last_and_new.T, self.product_families):\n",
        "      group[family] = diff_inv(ts, lag=self.diff_period)[2*self.diff_period:]\n",
        "\n",
        "  def _fit_transform_scaling(self, store, group):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1), clip=True)\n",
        "    self.scalers[store] = scaler\n",
        "    group[self.product_families] = scaler.fit_transform(\n",
        "         group[self.product_families])\n",
        "\n",
        "  def _transform_scaling(self, store, group):\n",
        "    group[self.product_families] = self.scalers[store].transform(\n",
        "        group[self.product_families])\n",
        "\n",
        "  def _inverse_transform_scaling(self, store, group):\n",
        "    group[self.product_families] = self.scalers[store].inverse_transform(\n",
        "      group[self.product_families])\n",
        "\n",
        "  def _fit_transform_lag(self, store, group):\n",
        "    period = self.lag_period\n",
        "    group[self.feat_lag] = group[self.product_families].shift(periods=period)\n",
        "    self.lag_last_known[store] = group[self.product_families].iloc[period:]\n",
        "    group.iloc[period:, group.columns.get_loc(\n",
        "        'is_used_for_diff_lag')] = True\n",
        "  def _transform_lag(self, store, group):\n",
        "    lag = self.lag_last_known[store]\n",
        "    group[self.feat_lag] = lag.iloc[-len(group):].values\n",
        "\n",
        "  def fit_transform(self, df):\n",
        "    transformed_data = []\n",
        "    df['is_used_for_diff_lag'] = False\n",
        "    for store, group in df.groupby(self.groupby_column):\n",
        "      group = group.copy()\n",
        "      if self.power_transformers is not None:\n",
        "        self._fit_transform_power(store, group)\n",
        "      if self.apply_diff:\n",
        "        self._fit_transform_differencing(store, group)\n",
        "      if self.scalers is not None:\n",
        "        self._fit_transform_scaling(store, group)\n",
        "      if self.derive_lags:\n",
        "        self._fit_transform_lag(store, group)\n",
        "      transformed_data.append(group)\n",
        "    return self._combine_group_data(transformed_data)\n",
        "\n",
        "  def remove_first_lag_diff(self, df):\n",
        "    days_to_remove = max(self.diff_period, self.lag_period)\n",
        "    if days_to_remove > 0:\n",
        "      second_period_start = df.date.min() + pd.DateOffset(\n",
        "          days=days_to_remove)\n",
        "      df = df.drop(index=df[df.date < second_period_start].index)\n",
        "    return df\n",
        "\n",
        "  def transform_lag_unseen(self, df):\n",
        "    if not self.derive_lags:\n",
        "      print('Derive lags is not chosen or lag_period zero! Returning original df')\n",
        "      return df\n",
        "    else:\n",
        "      lags = []\n",
        "      for store, group in df.groupby(self.groupby_column):\n",
        "        group = group.copy()\n",
        "        if len(group) > self.lag_period:\n",
        "          print(f'Period smaller than number of days for store {store}! Cannot derive lagged features for unseen data!')\n",
        "        else:\n",
        "          self._transform_lag(store, group)\n",
        "        lags.append(group)\n",
        "      return self._combine_group_data(lags)\n",
        "\n",
        "  def inverse_transform(self, df, unseen: bool):\n",
        "      inverse_data = []\n",
        "      for store, group in df.groupby(self.groupby_column):\n",
        "        group = group.copy()\n",
        "        if self.scalers is not None:\n",
        "          self._inverse_transform_scaling(store, group)\n",
        "        if self.apply_diff:\n",
        "          if unseen:\n",
        "            self._inverse_transform_differencing_unseen(store, group)\n",
        "          else:\n",
        "            self._inverse_transform_differencing_training(store, group)\n",
        "        if self.power_transformers is not None:\n",
        "          self._inverse_transform_power(store, group)\n",
        "        inverse_data.append(group)\n",
        "      return self._combine_group_data(inverse_data)\n",
        "\n",
        "  def transform_unseen(self, df):\n",
        "    transformed_data = []\n",
        "    for store, group in df.groupby(self.groupby_column):\n",
        "      group = group.copy()\n",
        "      if self.power_transformers is not None:\n",
        "        self._transform_power(store, group)\n",
        "      if self.apply_diff:\n",
        "        self._transform_differencing_unseen(store, group)\n",
        "      if self.scalers is not None:\n",
        "        self._transform_scaling(store, group)\n",
        "      if self.derive_lags:\n",
        "        if len(group) > self.lag_period:\n",
        "          print(f'Period smaller than number of days for store {store}! Cannot derive lagged features for unseen data!')\n",
        "        else:\n",
        "          self._transform_lag(store, group)\n",
        "      transformed_data.append(group)\n",
        "    return self._combine_group_data(transformed_data)\n",
        "\n",
        "def split_by_date(df, month, year=2017, day=1):\n",
        "  split = df.date >= pd.Timestamp(year=year, month=month, day=day)\n",
        "  return df[~split].copy(), df[split].copy()\n",
        "\n",
        "def test_store_wise_transformer(df, cols, split_month=6, split_day=1,\n",
        "                                **kwargs):\n",
        "  store_transformer = StorewiseTransformer(cols, **kwargs)\n",
        "  train, val = split_by_date(df, month=split_month, day=split_day)\n",
        "  print(train.shape)\n",
        "  train_trans = store_transformer.fit_transform(train)\n",
        "  print(train_trans.shape)\n",
        "  print(val.shape)\n",
        "  val_trans = store_transformer.transform_unseen(val)\n",
        "  print(val.shape)\n",
        "  _, train_trans_inv = split_by_date(train_trans, month=1)\n",
        "  train_trans_inv = store_transformer.inverse_transform(train_trans_inv, unseen=False)\n",
        "  print('Null values after inverse transformation of train (after split_month):')\n",
        "  print(train_trans_inv[train_trans_inv.isna().any(axis=1)])\n",
        "  val_trans = store_transformer.inverse_transform(val_trans, unseen=True)\n",
        "  print(\"Null values after inverse transformation of unseen data:\", val_trans.isna().sum().sum())\n",
        "  notequal = ((val[cols] - val_trans[cols]).abs() > 1e-2).any(axis=1)\n",
        "  return train_trans, val_trans, val[notequal][['date', 'store_nbr']]"
      ],
      "metadata": {
        "id": "xtZ-CS-qXCxC",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:41:03.756878Z",
          "iopub.execute_input": "2024-07-03T20:41:03.757298Z",
          "iopub.status.idle": "2024-07-03T20:41:04.084153Z",
          "shell.execute_reply.started": "2024-07-03T20:41:03.757267Z",
          "shell.execute_reply": "2024-07-03T20:41:04.083328Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction\n"
      ],
      "metadata": {
        "id": "yks_A1ZtXdFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Daily: Fourier Features and filled Oil Prices"
      ],
      "metadata": {
        "id": "u3FF1C4VJBYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.transformations.series.date import DateTimeFeatures\n",
        "from sktime.transformations.series.fourier import FourierFeatures\n",
        "def create_date_features(start_date=df_train.date.min(),\n",
        "                         end_date=df_test.date.max(),):\n",
        "  # Create a complete date range for the data\n",
        "  date_range = pd.date_range(start=start_date, end=end_date)\n",
        "  df_date = pd.DataFrame(date_range, index=date_range, columns=['date'])\n",
        "  date_features = DateTimeFeatures(ts_freq='D', keep_original_columns=True, manual_selection=[\n",
        "      'year', 'month_of_year', 'day_of_month', 'is_weekend', 'day_of_week'])\n",
        "  df_date = date_features.fit_transform(df_date)\n",
        "  fourier_features = FourierFeatures(sp_list=['W', 'M', 'Y'],\n",
        "                                     fourier_terms_list=[2, 2, 2],\n",
        "                                     keep_original_columns=True, freq='D')\n",
        "  df_date = fourier_features.fit_transform(df_date)\n",
        "  # Add payment date features\n",
        "  df_date['is_payment_date'] = df_date['day_of_month'].isin([15]) | (\n",
        "      df_date['day_of_month'] == df_date.index.to_series().dt.days_in_month)\n",
        "  df_date['is_post_payment_date'] = (df_date['is_payment_date'].shift(1) |\n",
        "                                   df_date['is_payment_date'].shift(2) |\n",
        "                                   df_date['is_payment_date'].shift(3)).fillna(False)\n",
        "  return df_date\n",
        "\n",
        "def extract_filled_oil_prices(df_date, oil):\n",
        "  df_oil = df_date.merge(oil, on='date', how='left', validate='m:1')\n",
        "  # Fill missing values using interpolation\n",
        "  df_oil['oil_price'] = df_oil['oil_price'].interpolate(method='linear').bfill()\n",
        "  return df_oil\n",
        "df = create_date_features()\n",
        "fourier_columns = [col for col in list(df.columns) if col.startswith('sin') or col.startswith('cos')]\n",
        "print(fourier_columns)"
      ],
      "metadata": {
        "id": "T5Hv8pUnCPDH",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:41:09.256027Z",
          "iopub.execute_input": "2024-07-03T20:41:09.257077Z",
          "iopub.status.idle": "2024-07-03T20:41:11.530533Z",
          "shell.execute_reply.started": "2024-07-03T20:41:09.257031Z",
          "shell.execute_reply": "2024-07-03T20:41:11.529538Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract regional holiday feature per store"
      ],
      "metadata": {
        "id": "GAotqKWdLXAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_holidays(df_holy, df_store):\n",
        "  # Remove transferred holidays\n",
        "  holidays = df_holy[df_holy['transferred'] == False].copy()\n",
        "  # Create boolean features\n",
        "  holidays['is_holiday'] = holidays['holiday_type'] == 'Holiday'\n",
        "  holidays['is_event'] = holidays['holiday_type'] == 'Event'\n",
        "  holidays['is_free'] = holidays['holiday_type'].isin(['Additional', 'Bridge', 'Transfer'])\n",
        "  holidays = holidays.drop(columns=['holiday_type', 'transferred', 'description'])\n",
        "  store = df_store[['store_nbr', 'city', 'state']]\n",
        "  national = pd.merge(holidays[holidays.locale == 'National'], store, how='cross')\n",
        "  regional = pd.merge(holidays[holidays.locale == 'Regional'], store, left_on='locale_name', right_on='state', how='left')\n",
        "  locale = pd.merge(holidays[holidays.locale == 'Local'], store, left_on='locale_name', right_on='city', how='left')\n",
        "  holidays = pd.concat([national, regional, locale], axis=0).drop(\n",
        "      columns=['locale', 'locale_name', 'city', 'state']).drop_duplicates()\n",
        "  # smash duplicate boolean flags together - loop since len(duplicates)==248\n",
        "  duplicates = holidays.duplicated(subset=['date', 'store_nbr'], keep=False)\n",
        "  date, store_nbr, is_holiday, is_event, is_free = [], [], [], [], []\n",
        "  for (d, s), g in holidays[duplicates].groupby(['date', 'store_nbr']):\n",
        "    date.append(d)\n",
        "    store_nbr.append(s)\n",
        "    is_holiday.append(g['is_holiday'].any())\n",
        "    is_event.append(g['is_event'].any())\n",
        "    is_free.append(g['is_free'].any())\n",
        "  unique = pd.DataFrame(data=dict(date=date, store_nbr=store_nbr, is_holiday=is_holiday, is_event=is_event, is_free=is_free))\n",
        "  return sort_df(pd.concat((holidays[~duplicates], unique)))\n",
        "extract_holidays(df_holy, df_store).head()"
      ],
      "metadata": {
        "id": "xg96fQSoW7W-",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:41:11.595458Z",
          "iopub.execute_input": "2024-07-03T20:41:11.596577Z",
          "iopub.status.idle": "2024-07-03T20:41:11.681424Z",
          "shell.execute_reply.started": "2024-07-03T20:41:11.596545Z",
          "shell.execute_reply": "2024-07-03T20:41:11.680388Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Store Size from Mean Number of transactions"
      ],
      "metadata": {
        "id": "sQ5Ss5qiRqLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def extract_store_size(transactions):\n",
        "  # Calculate the average number of transactions per store\n",
        "  transactions = transactions.groupby('store_nbr')[\n",
        "      'transactions'].mean().reset_index()\n",
        "  # Create numerical store_size feature based on scaled transaction means\n",
        "  transactions['store_size'] = MinMaxScaler().fit_transform(\n",
        "      transactions.transactions.values.reshape(-1, 1))\n",
        "  # Define thresholds for categorizing stores into different sizes\n",
        "  thresholds = {\n",
        "      'tiny': transactions['transactions'].quantile(0.09),\n",
        "      'small': transactions['transactions'].quantile(0.33),\n",
        "      'medium': transactions['transactions'].quantile(0.66),\n",
        "      'large': transactions['transactions'].quantile(0.9),\n",
        "  }\n",
        "\n",
        "  # Create a new ordinal categorical feature 'store_size' based on the thresholds\n",
        "  def categorize_store_size(avg_transactions):\n",
        "      if avg_transactions <= thresholds['tiny']:\n",
        "          return 'tiny'\n",
        "      elif avg_transactions <= thresholds['small']:\n",
        "          return 'small'\n",
        "      elif avg_transactions <= thresholds['medium']:\n",
        "          return 'medium'\n",
        "      elif avg_transactions <= thresholds['large']:\n",
        "          return 'large'\n",
        "      else:\n",
        "          return 'giant'\n",
        "\n",
        "  transactions['store_size_group'] = transactions[\n",
        "      'transactions'].apply(categorize_store_size)\n",
        "  transactions['store_size_group'] = pd.Categorical(\n",
        "      transactions['store_size_group'], categories=[\n",
        "          'tiny', 'small', 'medium', 'large', 'giant'], ordered=True)\n",
        "  transactions.drop(columns='transactions', inplace=True)\n",
        "  return transactions\n",
        "\n",
        "def show_store_size(df):\n",
        "  df = extract_store_size(df)\n",
        "  # Plotting\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  order = df.sort_values('store_size').store_nbr\n",
        "  sns.barplot(x='store_nbr', y='store_size', hue='store_size_group',\n",
        "              data=df, dodge=False, order=order)\n",
        "  plt.title('Store Size by Store Number')\n",
        "  plt.xlabel('Store Number')\n",
        "  plt.ylabel('Normalized Store Size')\n",
        "  plt.legend(title='Store Size Group')\n",
        "  plt.show()\n",
        "\n",
        "show_store_size(df_trans)"
      ],
      "metadata": {
        "id": "OCTh7vxZRyk_",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:41:44.64481Z",
          "iopub.execute_input": "2024-07-03T20:41:44.645609Z",
          "iopub.status.idle": "2024-07-03T20:41:44.657585Z",
          "shell.execute_reply.started": "2024-07-03T20:41:44.645573Z",
          "shell.execute_reply": "2024-07-03T20:41:44.656685Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract City and State Size from Census Data"
      ],
      "metadata": {
        "id": "butFZAjSG8jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "\n",
        "def extract_census_date():\n",
        "  census_city = StringIO(\"\"\"\n",
        "city,population_city\n",
        "Amaguaña,10628\n",
        "Ambato,177316\n",
        "Anconcito,15033\n",
        "Arenillas,21524\n",
        "Atacames,18948\n",
        "Atuntaqui (Antonio Ante),25115\n",
        "Azogues,35763\n",
        "Babahoyo,98251\n",
        "Bahía de Caráquez (Sucre),22209\n",
        "Balao,12850\n",
        "Balzar,32744\n",
        "Baños de Agua Santa,14100\n",
        "Buena Fé (San Jacinto de Buena Fé),46779\n",
        "Calceta (Bolivar),20011\n",
        "Calderón,249941\n",
        "Camilo Ponce Enríquez,10015\n",
        "Cañar,13148\n",
        "Cariamanga,13175\n",
        "Catamayo,27026\n",
        "Cayambe,44559\n",
        "Charapotó,11879\n",
        "Chaupitena (incl. Tena and Santa Isabel),16066\n",
        "Chone,54629\n",
        "Conocoto,121984\n",
        "Cotacachi,10526\n",
        "Crucita,10318\n",
        "Cuenca,361524\n",
        "Cumandá,11442\n",
        "Cumbayá,41740\n",
        "Daule,46438\n",
        "Durán (Eloy Alfaro),295211\n",
        "El Carmen,52366\n",
        "El Guabo,26635\n",
        "El Salitre,13571\n",
        "El Triunfo,41042\n",
        "Esmeraldas,155487\n",
        "Gualaceo (Santiago de Gualaceo),13843\n",
        "Guaranda,30755\n",
        "Guayaquil,2650288\n",
        "Guayllabamba,13336\n",
        "Huachi Grande,11335\n",
        "Huaquillas,56021\n",
        "Ibarra,157941\n",
        "Izamba,13365\n",
        "Jaramijó,28397\n",
        "Jipijapa,45382\n",
        "José Luis Tamayo (Muey),35833\n",
        "La Aurora (incl. San Antonio),116593\n",
        "La Concordia,35474\n",
        "La Joya de los Sachas,16023\n",
        "Libertad,112154\n",
        "La Maná,31740\n",
        "La Puntilla,55357\n",
        "Latacunga,77267\n",
        "La Troncal,39600\n",
        "La Unión,10480\n",
        "Llano Chico,14741\n",
        "Loja,203496\n",
        "Lomas de Sargentillo,16603\n",
        "Macará,12454\n",
        "Macas (Morona),22398\n",
        "Machachi,24188\n",
        "Machala,288072\n",
        "Manta,258697\n",
        "Milagro,159970\n",
        "Montalvo,16248\n",
        "Montecristi,71066\n",
        "Naranjal,39323\n",
        "Naranjito,34664\n",
        "Nobol,10010\n",
        "Nueva Loja (Lago Agrio),55627\n",
        "Otavalo,41718\n",
        "Palestina,10392\n",
        "Pasaje,60147\n",
        "Pedernales,27068\n",
        "Pedro Carbo,24882\n",
        "Pelileo,11403\n",
        "Pifo,12868\n",
        "Píllaro,9816\n",
        "Piñas,18482\n",
        "Playas,48156\n",
        "Pomasqui,28724\n",
        "Portoviejo,244129\n",
        "Posorja,30886\n",
        "Puerto Ayora,12696\n",
        "Puerto Baquerizo Moreno (San Cristóbal),7290\n",
        "Puerto Francisco de Orellana (Ciudad Coco),51281\n",
        "Puerto López,12598\n",
        "Pujilí,16152\n",
        "Puyo,33325\n",
        "Quevedo,177792\n",
        "Quito,1763275\n",
        "Ricaurte,13378\n",
        "Riobamba,177213\n",
        "Rocafuerte,11848\n",
        "Rosa Zárate (Quinindé),31120\n",
        "Salcedo (San Miguel),16751\n",
        "Salinas,35066\n",
        "Samborondón,17068\n",
        "San Antonio de Ibarra,13626\n",
        "San Antonio de Pichincha,45380\n",
        "San Carlos,22529\n",
        "San Gabriel (Montúfar),14497\n",
        "Sangolquí (Rumiñahui),96647\n",
        "San Juan,12819\n",
        "San Lorenzo,28491\n",
        "Santa Ana de Vuelta Larga,11918\n",
        "Santa Elena,54565\n",
        "Santa Lucía,10924\n",
        "Santa Rosa,56842\n",
        "Santo Domingo,334826\n",
        "Santo Domingo de Cutuglahua (Cutuglahua),15356\n",
        "San Vicente,10404\n",
        "Saquisilí,9883\n",
        "Shushufindi,16328\n",
        "Sucúa,10846\n",
        "Tabacundo,13019\n",
        "Tarifa,10584\n",
        "Tena,29724\n",
        "Tonsupa,11526\n",
        "Tosagua,11317\n",
        "Tulcán,56719\n",
        "Tumbaco,70789\n",
        "Valencia,22996\n",
        "Velasco Ibarra (El Empalme),41778\n",
        "Ventanas,41531\n",
        "Vinces,35064\n",
        "Virgen de Fátima,15066\n",
        "Yaguachi (San Jacinto de Yaguachi),22972\n",
        "Yantzaza,13335\n",
        "Yaruquí,11276\n",
        "Zamora,17584\n",
        "Zaruma,10005\n",
        "\"\"\")\n",
        "  census_state = StringIO(\"\"\"\n",
        "state,population_state\n",
        "Azuay,801609\n",
        "Bolivar,199078\n",
        "Cañar,227578\n",
        "Carchi,172828\n",
        "Chimborazo,471933\n",
        "Cotopaxi,470210\n",
        "El Oro,714592\n",
        "Esmeraldas,553900\n",
        "Galápagos,28583\n",
        "Guayas,4391923\n",
        "Imbabura,469879\n",
        "Loja,485421\n",
        "Los Rios,898652\n",
        "Manabi,1592840\n",
        "Morona Santiago,192508\n",
        "Napo,131675\n",
        "Orellana,182166\n",
        "Pastaza,111915\n",
        "Pichincha,3089473\n",
        "Santa Elena,385735\n",
        "Santo Domingo de los Tsachilas,492969\n",
        "Sucumbíos,199014\n",
        "Tungurahua,563532\n",
        "Zamora Chinchipe,110973\n",
        "\"\"\")\n",
        "  city = pd.read_table(census_city, sep=',', dtype=dict(city='category', population_city=int))\n",
        "  state = pd.read_table(census_state, sep=',', dtype=dict(state='category', population_state=int))\n",
        "  return city, state"
      ],
      "metadata": {
        "id": "XB2W6WPlW1YL",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:41:50.56477Z",
          "iopub.execute_input": "2024-07-03T20:41:50.565457Z",
          "iopub.status.idle": "2024-07-03T20:41:50.575833Z",
          "shell.execute_reply.started": "2024-07-03T20:41:50.565424Z",
          "shell.execute_reply": "2024-07-03T20:41:50.57493Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract whether a store was open based on sum over sales"
      ],
      "metadata": {
        "id": "A78Z-Df2mXoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_closed_n_days_in_group(group, n_days):\n",
        "  is_closed = ~ group.is_open\n",
        "  num_days_closed = is_closed.astype(int).rolling(\n",
        "      window=n_days, min_periods=1, center=True\n",
        "  ).sum()\n",
        "  group['is_closed_n_days'] = num_days_closed == n_days\n",
        "  return group\n",
        "\n",
        "def extract_is_open(df):\n",
        "  df = sort_df(df)\n",
        "  df['is_open'] = df['sales_MM'] > 0\n",
        "  return df\n",
        "\n",
        "def extract_open_closed_n_days(df, n_days=3):\n",
        "  # Detect whether the store was closed for more than n days in a row\n",
        "  df = df.groupby('store_nbr').apply(\n",
        "      extract_closed_n_days_in_group, n_days=n_days).reset_index(drop=True)\n",
        "  return sort_df(df)\n",
        "\n",
        "def show_closed_stores(df, store=18, year=2016):\n",
        "  df = extract_is_open(df)\n",
        "  df = extract_open_closed_n_days(df)\n",
        "  fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(21, 7))\n",
        "  df[~df.is_open].groupby('store_nbr')['date'].count().plot.bar(\n",
        "      xlabel='Store', ylabel='Number of Closed Days',\n",
        "      title='Number of Closed Days for each store (over all training years)',\n",
        "      ax=ax1\n",
        "  )\n",
        "  filter = (df.store_nbr == store) & (df['date'].dt.year == year)\n",
        "  df_store_year = df[filter]\n",
        "  sns.lineplot(\n",
        "      x='date', y='sales_MM', data=df_store_year[df_store_year.is_open],\n",
        "      marker='o', ax=ax2, color='blue', label='Open Days'\n",
        "  )\n",
        "  sns.scatterplot(\n",
        "      x='date', y='sales_MM', data=df_store_year[\n",
        "          ~df_store_year.is_open & ~df_store_year.is_closed_n_days],\n",
        "      color='red', marker='X', ax=ax2, label='Closed Days'\n",
        "  )\n",
        "  sns.scatterplot(\n",
        "      x='date', y='sales_MM', data=df_store_year[df_store_year.is_closed_n_days],\n",
        "      color='green', marker='D', ax=ax2, label=f'Closed > 2 Days'\n",
        "  )\n",
        "  ax2.set_title(f'Sum of scaled sales for store {store} and year {year}')\n",
        "  ax2.legend(title='Store Status', loc='best')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "show_closed_stores(df_train, store=51, year=2017)"
      ],
      "metadata": {
        "id": "-6OrCNeSxJzN",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:42:00.545745Z",
          "iopub.execute_input": "2024-07-03T20:42:00.54664Z",
          "iopub.status.idle": "2024-07-03T20:42:00.559293Z",
          "shell.execute_reply.started": "2024-07-03T20:42:00.546606Z",
          "shell.execute_reply": "2024-07-03T20:42:00.558277Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Z-Scores for Outlier Detection\n"
      ],
      "metadata": {
        "id": "pyigw5Qpaix2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_outliers_in_group(group, target_columns, num_col_outliers,\n",
        "                               exclude_closed_days):\n",
        "    threshold = 3\n",
        "    is_outlier_columns = [f'{col}_is_outlier' for col in target_columns]\n",
        "    for col, is_outlier_col in zip(target_columns, is_outlier_columns):\n",
        "      target = group[col]\n",
        "      if exclude_closed_days:\n",
        "        target = group[group.is_open][col]\n",
        "      else:\n",
        "        target = group[col]\n",
        "      target_std = target.std()\n",
        "      if target_std == 0.0:\n",
        "        group[is_outlier_col] = True\n",
        "      else:\n",
        "        z_score = (group[col] - target.mean()) / target_std\n",
        "        group[is_outlier_col] = z_score.abs() > threshold\n",
        "    # Mark as outliers if Z-score is above the threshold for more than num_col_outliers\n",
        "    group['is_outlier'] = group[is_outlier_columns].sum(axis=1) > num_col_outliers\n",
        "    return group\n",
        "\n",
        "def outlier_detection(df, target_columns=sales_columns, num_col_outliers=round(\n",
        "    len(sales_columns) / 2), exclude_closed_days=True):\n",
        "  df = df.groupby(['store_nbr']).apply(\n",
        "    identify_outliers_in_group, target_columns=target_columns,\n",
        "    num_col_outliers=num_col_outliers, exclude_closed_days=exclude_closed_days).reset_index(drop=True)\n",
        "  return sort_df(df)\n",
        "\n",
        "def plot_outliers(df):\n",
        "  df = extract_is_open(df)\n",
        "  df = outlier_detection(df[df.is_open])\n",
        "  num_out = sum(df.is_outlier)\n",
        "  num_total = df.shape[0]\n",
        "  df[df.is_outlier].groupby(['store_nbr'])['date'].count().plot.bar()\n",
        "  plt.title(f'{num_out} outliers - total {num_total} - {(100 * num_out / num_total):.2f} %')\n",
        "  plt.ylabel('Number of Outliers')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "plot_outliers(df_train)"
      ],
      "metadata": {
        "id": "ZstexNNaeoNH",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:42:08.844844Z",
          "iopub.execute_input": "2024-07-03T20:42:08.845666Z",
          "iopub.status.idle": "2024-07-03T20:42:08.864098Z",
          "shell.execute_reply.started": "2024-07-03T20:42:08.845624Z",
          "shell.execute_reply": "2024-07-03T20:42:08.862988Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Source Data Merging"
      ],
      "metadata": {
        "id": "X-U2mdZzFKYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SalesDataExtractor:\n",
        "  def __init__(self,\n",
        "               df_oil=df_oil,\n",
        "               df_holy=df_holy,\n",
        "               df_store=df_store,\n",
        "               df_trans=df_trans):\n",
        "    self.df_date_oil = extract_filled_oil_prices(create_date_features(),\n",
        "                                                 df_oil)\n",
        "    self.df_store = self.merge_store_data(df_store, df_trans)\n",
        "    self.df_holy = extract_holidays(df_holy, df_store)\n",
        "\n",
        "  @staticmethod\n",
        "  def merge_store_data(store, transactions):\n",
        "    transactions = extract_store_size(transactions)\n",
        "    store = store.merge(transactions, on='store_nbr', how='left', validate='1:1')\n",
        "    city, state = extract_census_date()\n",
        "    store = store.merge(city, on='city', how='left', validate='m:1')\n",
        "    store = store.merge(state, on='state', how='left', validate='m:1')\n",
        "    return store\n",
        "\n",
        "  def merge_source_data(self, df):\n",
        "    df = df.merge(self.df_holy, on=['date', 'store_nbr'], how='left', validate='1:1')\n",
        "    df[['is_holiday', 'is_event', 'is_free']] = df[\n",
        "        ['is_holiday', 'is_event', 'is_free']].fillna(False)\n",
        "    df = df.merge(self.df_date_oil, on='date', how='left', validate='m:1')\n",
        "    df.is_free = df.is_free & df.is_weekend\n",
        "    df = df.drop(columns='is_weekend')\n",
        "    df = df.merge(self.df_store, on='store_nbr', how='left', validate='m:1')\n",
        "    for feat in ['city', 'state', 'store_nbr']: # dtype object after merging\n",
        "      df[feat] = pd.Categorical(df[feat])\n",
        "    return sort_df(df)\n",
        "\n",
        "SalesDataExtractor().merge_source_data(df_train).duplicated(\n",
        "    subset=['date', 'store_nbr']).sum()"
      ],
      "metadata": {
        "id": "8jq6kgikFN-8",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:42:12.531933Z",
          "iopub.execute_input": "2024-07-03T20:42:12.532701Z",
          "iopub.status.idle": "2024-07-03T20:42:13.512999Z",
          "shell.execute_reply.started": "2024-07-03T20:42:12.532665Z",
          "shell.execute_reply": "2024-07-03T20:42:13.512068Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mutual Information and Correlation\n",
        "\n"
      ],
      "metadata": {
        "id": "zLxWG8MUzLoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "def sorted_mutual_info(df, feature_lists, is_discrete_list, col_target):\n",
        "  y = df[col_target]\n",
        "  mi = []\n",
        "  for features, is_discrete in zip(feature_lists, is_discrete_list):\n",
        "    mutual_info_score = mutual_info_regression(df[features], y,\n",
        "                    discrete_features=is_discrete, random_state=SEED)\n",
        "    mutual_info = pd.Series({\n",
        "      feat: score for feat, score in zip(features, mutual_info_score)\n",
        "    }).sort_values(ascending=False)\n",
        "    mi.append(mutual_info)\n",
        "  return pd.concat(mi)\n",
        "\n",
        "def show_mutual_info_correlation(df, feat_date, feat_emb,\n",
        "                                 feat_bool, feat_numeric, families=['DAIRY_sales']):\n",
        "  _, df = split_by_date(df, month=1, year=2016)\n",
        "  df = extract_is_open(df)\n",
        "  df = SalesDataExtractor().merge_source_data(df)\n",
        "  feat_fourier = [col for col in list(df.columns) if col.startswith('sin') or col.startswith('cos')]\n",
        "  for family in families:\n",
        "    mutual_info = sorted_mutual_info(df,\n",
        "                      feature_lists = [feat_date, feat_fourier, feat_emb, feat_bool, feat_numeric],\n",
        "                      is_discrete_list = [True, False, True, True, False],\n",
        "                      col_target=family)\n",
        "    sns.barplot(x=mutual_info, y=mutual_info.index)\n",
        "    plt.xlim((0.0, 0.5))\n",
        "    plt.title(f'Mutual Information to {family}')\n",
        "    plt.xlabel('')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "  features = feat_date + feat_fourier + feat_bool + feat_numeric # exclude feat_emb (spearman needs ranking)\n",
        "  corr = df[features + families].corr(method='spearman')\n",
        "  fig, ax = plt.subplots(figsize=(17,17))\n",
        "  sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
        "              linewidths=0.5, ax=ax)\n",
        "\n",
        "show_mutual_info_correlation(\n",
        "    df_train,\n",
        "    feat_date=['year', 'day_of_month'],\n",
        "    feat_emb=['store_nbr', 'store_type', 'cluster', 'city', 'state', 'store_size_group'],\n",
        "    feat_bool=['is_payment_date', 'is_post_payment_date',\n",
        "                'is_holiday', 'is_event', 'is_free', 'is_open'],\n",
        "    feat_numeric=['store_size', 'oil_price', 'population_city', 'population_state'],\n",
        "    families=['GROCERY I_sales', 'DAIRY_sales', 'EGGS_sales', 'sales_MM']\n",
        ")"
      ],
      "metadata": {
        "id": "cEZIqlRpSgmk",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection"
      ],
      "metadata": {
        "id": "MY5LXkeGc9I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling and Encoding Pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "x9cZ345Q4AC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder\n",
        "\n",
        "class DataPipeline:\n",
        "  def __init__(self,\n",
        "      feat_minmax: list[str],\n",
        "      feat_onehot: list[str],\n",
        "      feat_ordinal: list[str],\n",
        "      feat_std: list[str],\n",
        "      feat_storewise: list[str],\n",
        "      filter_closed_outlier: bool,\n",
        "      feat_pass: list[str],\n",
        "      target_columns=sales_columns,\n",
        "      id_columns=id_columns,\n",
        "      **storewise_kwargs\n",
        "  ):\n",
        "    self.target_columns = target_columns\n",
        "    self.id_columns = id_columns\n",
        "    self.filter_train = filter_closed_outlier\n",
        "    if feat_storewise:\n",
        "      self.storewise_feat_transformer = StorewiseTransformer(feat_storewise,\n",
        "        **storewise_kwargs)\n",
        "      self.feat_lag = self.storewise_feat_transformer.feat_lag\n",
        "    else:\n",
        "      self.storewise_feat_transformer = None\n",
        "      self.feat_lag = []\n",
        "    self.target_transformer = StorewiseTransformer(target_columns,\n",
        "        **storewise_kwargs)\n",
        "    self.feat_lag = self.feat_lag + self.target_transformer.feat_lag\n",
        "    self.data_extractor = SalesDataExtractor()\n",
        "    self.feat_pass = feat_pass + feat_storewise + self.feat_lag\n",
        "    self.feat_transformer = ColumnTransformer(\n",
        "      transformers=[\n",
        "          ('minmax', MinMaxScaler(), feat_minmax), # float\n",
        "          ('ordinal', OrdinalEncoder(dtype=int), feat_ordinal), # int\n",
        "          ('onehot', OneHotEncoder(drop=None, sparse_output=False), feat_onehot),\n",
        "          ('std', StandardScaler(), feat_std), # float\n",
        "      ], sparse_threshold=0.0, verbose_feature_names_out=False, remainder='drop')\n",
        "    self.feat_transformer.set_output(transform='pandas')\n",
        "\n",
        "  def fit_transform(self, df):\n",
        "    df = sort_df(df)\n",
        "    df = extract_is_open(df)\n",
        "    if self.filter_train:\n",
        "      df = outlier_detection(df)\n",
        "    if self.storewise_feat_transformer is not None:\n",
        "      df = self.storewise_feat_transformer.fit_transform(df)\n",
        "    df = self.target_transformer.fit_transform(df)\n",
        "    # now we can savely drop NaNs that resulted from shift\n",
        "    df = self.target_transformer.remove_first_lag_diff(df)\n",
        "    df = self.data_extractor.merge_source_data(df)\n",
        "    if self.filter_train:\n",
        "      df = df[(df.is_open & ~df.is_outlier) | df.is_used_for_diff_lag]\n",
        "    df_preprocessed = self.feat_transformer.fit_transform(df)\n",
        "    return pd.concat((df_preprocessed, df[\n",
        "        self.feat_pass + self.target_columns]), axis=1)\n",
        "\n",
        "  def transform(self, df, include_sales: bool):\n",
        "    df = sort_df(df)\n",
        "    if self.storewise_feat_transformer is not None:\n",
        "      df = self.storewise_feat_transformer.transform_unseen(df)\n",
        "    if include_sales:\n",
        "      df = self.target_transformer.transform_unseen(df)\n",
        "    elif self.feat_lag:\n",
        "      df = self.target_transformer.transform_lag_unseen(df)\n",
        "    df = self.data_extractor.merge_source_data(df)\n",
        "    df_preprocessed = self.feat_transformer.transform(df)\n",
        "    if not self.filter_train: # add dummies\n",
        "      df_preprocessed['is_open'] = 1\n",
        "      df_preprocessed['is_outlier'] = 0\n",
        "    cols_to_pass = self.target_columns if include_sales else self.id_columns\n",
        "    return pd.concat((df_preprocessed, df[\n",
        "        self.feat_pass + cols_to_pass]), axis=1)\n",
        "\n",
        "  def inverse_transform_target(self, df, unseen: bool):\n",
        "    return self.target_transformer.inverse_transform(df, unseen=unseen)\n",
        "\n",
        "pipe = DataPipeline(\n",
        "  feat_minmax=['year', 'is_post_payment_date', 'is_holiday', 'is_free'],\n",
        "  feat_onehot=['store_type'],\n",
        "  feat_ordinal=['cluster', 'city', 'state'],\n",
        "  feat_std=['store_size', 'oil_price', 'population_city', 'population_state'],\n",
        "  feat_pass=fourier_columns+['date', 'store_nbr'],\n",
        "  feat_storewise=onpromotion_columns,\n",
        "  filter_closed_outlier=True,\n",
        "  apply_power=False,\n",
        "  apply_scaling=True,\n",
        "  diff_period=0,\n",
        "  lag_period=0\n",
        ")\n",
        "print(f'Lagged Features: {pipe.feat_lag}')\n",
        "print('NaN values after transforming training data: ',\n",
        "      pipe.fit_transform(df_train).isna().sum().sum())\n",
        "pipe.transform(df_test, include_sales=False).isna().sum().sum()"
      ],
      "metadata": {
        "id": "qZT2AdoC7MPs",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:44:15.872887Z",
          "iopub.execute_input": "2024-07-03T20:44:15.873299Z",
          "iopub.status.idle": "2024-07-03T20:44:22.965743Z",
          "shell.execute_reply.started": "2024-07-03T20:44:15.873264Z",
          "shell.execute_reply": "2024-07-03T20:44:22.964834Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "61jyRFj80LYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, df_ana = split_by_date(df_train, year=2016, month=1) # 592 days\n",
        "ana_families=['DAIRY_sales', 'EGGS_sales', 'GROCERY I_sales', 'GROCERY II_sales']\n",
        "ana_stores=[25, 32, 4, 51, 7, 43] # follows store size ordering (51 started in 04 17)\n",
        "df_ana = SalesDataExtractor().merge_source_data(df_ana)\n",
        "df_ana = extract_is_open(df_ana)\n",
        "df_ana[df_ana.store_nbr.isin(ana_stores)][['date', 'store_nbr'] + ana_families].tail()"
      ],
      "metadata": {
        "id": "z1OzGdDd_Gi4",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:44:27.424739Z",
          "iopub.execute_input": "2024-07-03T20:44:27.425444Z",
          "iopub.status.idle": "2024-07-03T20:44:28.227496Z",
          "shell.execute_reply.started": "2024-07-03T20:44:27.425411Z",
          "shell.execute_reply": "2024-07-03T20:44:28.226421Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoregression: PACF and Differencing (ADF-Test)"
      ],
      "metadata": {
        "id": "fHtiYgLV9kvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import STL, MSTL\n",
        "from pmdarima.arima import ADFTest\n",
        "from pmdarima.utils import plot_pacf\n",
        "\n",
        "def get_stl_decomposition(df, columns, periods):\n",
        "  df = df[['date', 'store_nbr'] + columns].copy()\n",
        "  for col in columns:\n",
        "    stl = MSTL(df[col], periods=periods)\n",
        "    result = stl.fit()\n",
        "    df[f'{col}_trend'] = result.trend\n",
        "    if isinstance(periods, int):\n",
        "      df[f'{col}_seasonal_{periods}'] = result.seasonal\n",
        "    else:\n",
        "      for i, p in enumerate(periods):\n",
        "        df[f'{col}_seasonal_{p}'] = result.seasonal.values[:, i]\n",
        "    df[f'{col}_resid'] = result.resid\n",
        "  return df\n",
        "\n",
        "def plot_decomposition(df, family_to_plot, periods=(7, 30)):\n",
        "  result = get_stl_decomposition(df, columns=[family_to_plot], periods=periods)\n",
        "  plt.figure(figsize=(30,10))\n",
        "  plt.subplot(411)\n",
        "  plt.plot(result['date'], result[f'{family_to_plot}'])\n",
        "  plt.title(f'{family_to_plot} Observed')\n",
        "  plt.subplot(412)\n",
        "  plt.plot(result['date'], result[f'{family_to_plot}_trend'])\n",
        "  plt.title('Trend')\n",
        "  plt.subplot(413)\n",
        "  if isinstance(periods, int):\n",
        "    periods = [periods]\n",
        "  for p in periods:\n",
        "    plt.plot(result['date'], result[f'{family_to_plot}_seasonal_{p}'], label=p)\n",
        "  plt.legend()\n",
        "  plt.title('Seasonal')\n",
        "  plt.subplot(414)\n",
        "  plt.plot(result['date'], result[f'{family_to_plot}_resid'])\n",
        "  plt.title(f'{family_to_plot} Residual')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def augmented_dickey_fuller_test(df, columns):\n",
        "  should_diff, should_diff_p = [], []\n",
        "  adf = ADFTest()\n",
        "  for col in columns:\n",
        "    x = df[col]\n",
        "    pval, sig = adf.should_diff(x)\n",
        "    should_diff.append(sig)\n",
        "    should_diff_p.append(pval)\n",
        "  return pd.DataFrame(index=columns, data={\n",
        "    'Needs Differencing': should_diff, 'Diff p-value': should_diff_p,\n",
        "  })\n",
        "\n",
        "def show_decomposition_pacf_adf(df, family=ana_families[0],\n",
        "                                store_nbr=ana_stores[2], period=7):\n",
        "  df = df[(df.store_nbr == store_nbr)]\n",
        "  plot_decomposition(df, family_to_plot=family, periods=period)\n",
        "  plot_pacf(df[family])\n",
        "  return augmented_dickey_fuller_test(df, sales_columns)\n",
        "\n",
        "show_decomposition_pacf_adf(df_ana)"
      ],
      "metadata": {
        "id": "OpAQZsUk9uzc",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average Sales per Feature"
      ],
      "metadata": {
        "id": "rRM6mI-4fj5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feat_date = ['day_of_month', 'day_of_week', 'month_of_year']\n",
        "features = ['store_nbr', 'city', 'state', 'store_type', 'cluster', 'store_size_group'] + feat_date\n",
        "\n",
        "def beauty_ax(ax_i, title):\n",
        "  ax_i.set_title(title, fontweight=\"bold\")\n",
        "  ax_i.set_xlabel('')\n",
        "  for tick in ax_i.get_xticklabels():\n",
        "    tick.set_rotation(90)\n",
        "\n",
        "def show_sales_per_feature(df, features, target='sales_MM'):\n",
        "  df = df[df.is_open]\n",
        "  fig, ax = plt.subplots(nrows=len(features),\n",
        "                         ncols=1, figsize=(25, len(features) * 4))\n",
        "  for i, feat in enumerate(features):\n",
        "    # Calculate the average sales per year for each feature\n",
        "    ax_i = ax[i]\n",
        "    ordering = df.groupby(feat, as_index=True)[\n",
        "        target].mean().sort_values(ascending=False).index\n",
        "    df_avg_per_year = df.groupby(\n",
        "        [feat], as_index=False)[target].mean()\n",
        "    sns.barplot(data=df_avg_per_year, x=feat, y=target,\n",
        "                order=ordering, ax=ax_i)\n",
        "    beauty_ax(ax_i, f'Mean of {target} per {feat}')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "show_sales_per_feature(df_ana, features, target=ana_families[0])"
      ],
      "metadata": {
        "id": "55fH5OZJsai9",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Daily Sales vs Oil Price per product category"
      ],
      "metadata": {
        "id": "NoJZ3P7P1Xy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sales_against_oil(df, families=ana_families, rolling_window_len=7):\n",
        "  df = df.groupby('date').agg(\n",
        "    {col: 'mean' for col in families + ['oil_price']}\n",
        "  ).rolling(rolling_window_len).mean()\n",
        "  nrows = len(families)\n",
        "  fig, ax = plt.subplots(nrows=nrows, ncols=1, figsize=(20,4*nrows))\n",
        "  ax[0].set_title(f'Mean of Sales vs Oil Price: {rolling_window_len} days rolling mean')\n",
        "  for i in range(nrows):\n",
        "    ax_i = ax[i]\n",
        "    y = families[i]\n",
        "    sns.lineplot(x='date', y=y, data=df,\n",
        "              color='blue', label=f'{y}',\n",
        "              marker='o', ax=ax_i)\n",
        "    ax_i.legend(loc='upper left')\n",
        "    ax2 = ax_i.twinx()\n",
        "    sns.lineplot(x='date', y='oil_price', data=df,\n",
        "              color='red', label='Daily Oil Price',\n",
        "              marker='o', ax=ax2)\n",
        "    ax2.legend(loc='lower right')\n",
        "  plt.show()\n",
        "\n",
        "show_sales_against_oil(df_ana, rolling_window_len=7,\n",
        "                       families=ana_families + ['sales_MM'])"
      ],
      "metadata": {
        "id": "8aFUmnL81jMO",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Daily sales vs Promotions per product category and Store"
      ],
      "metadata": {
        "id": "Lt4aRJh1glmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sales_against_promotions(df, rolling_window_len=7,\n",
        "                                  families=ana_families, stores=ana_stores):\n",
        "  promos = [sale_column.replace('_sales', '_onpromotion') for sale_column in families]\n",
        "  df = df[df.store_nbr.isin(stores)]\n",
        "  nrows = len(stores)\n",
        "  ncols = len(families)\n",
        "  fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10*ncols,4*nrows))\n",
        "  for i, store in enumerate(stores):\n",
        "    df_store = df[df.store_nbr == store]\n",
        "    df_store = df_store.groupby('date').agg(\n",
        "      {col: 'sum' for col in families + promos}\n",
        "    ).rolling(rolling_window_len).mean()\n",
        "    ax[i, 0].set_title(f'Store {store} Sales vs Promotions: {rolling_window_len} days rolling mean')\n",
        "    for j, (fam, promo) in enumerate(zip(families, promos)):\n",
        "      ax_i = ax[i,j]\n",
        "      sns.lineplot(x='date', y=fam, data=df_store,\n",
        "                color='blue', label=f'{fam}',\n",
        "                marker='o', ax=ax_i)\n",
        "      ax_i.legend(loc='upper left')\n",
        "      ax2 = ax_i.twinx()\n",
        "      sns.lineplot(x='date', y=promo, data=df_store,\n",
        "                color='red', label=f'{promo}',\n",
        "                marker='o', ax=ax2)\n",
        "      ax2.legend(loc='lower right')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "show_sales_against_promotions(df_ana, rolling_window_len=7)"
      ],
      "metadata": {
        "id": "H_8d9BGpijTB",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis Insights Summary"
      ],
      "metadata": {
        "id": "3Kh7InphKBtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Multiple Regression Model"
      ],
      "metadata": {
        "id": "6KvKwcGlpSaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building"
      ],
      "metadata": {
        "id": "SqrfPM4ikeby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.optim as optim\n",
        "if torch.cuda.is_available():\n",
        "  print('Using cuda on GPU')\n",
        "  device = torch.device('cuda')\n",
        "  ! nvidia-smi\n",
        "else:\n",
        "  device = 'cpu'\n",
        "pl.seed_everything(SEED)"
      ],
      "metadata": {
        "id": "Se_4ehDu6T__",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:44:43.838745Z",
          "iopub.execute_input": "2024-07-03T20:44:43.839551Z",
          "iopub.status.idle": "2024-07-03T20:44:45.012026Z",
          "shell.execute_reply.started": "2024-07-03T20:44:43.839515Z",
          "shell.execute_reply": "2024-07-03T20:44:45.010983Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embed categorical features"
      ],
      "metadata": {
        "id": "-Nat-EYZePL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StoreEmbeddingLayer(nn.Module):\n",
        "  def __init__(self, emb_in: list[int], emb_out: list[int], num_feat_aux: int):\n",
        "    super(StoreEmbeddingLayer, self).__init__()\n",
        "    self.embedding_layers = nn.ModuleList([\n",
        "        nn.Embedding(n_in, n_out) for n_in, n_out in zip(emb_in, emb_out)\n",
        "    ])\n",
        "    self.total_num_in = sum(emb_out) + num_feat_aux\n",
        "    self.total_num_out = round(self.total_num_in / 2)\n",
        "    self.fc = nn.Sequential(\n",
        "            nn.Linear(self.total_num_in, self.total_num_out),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(self.total_num_out),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "  def forward(self, emb_data_list: list[torch.tensor], aux_data: torch.tensor):\n",
        "    emb_data_list = [emb_layer(x) for emb_layer, x in zip(self.embedding_layers, emb_data_list)]\n",
        "    emb_data_list.append(aux_data)\n",
        "    return self.fc(torch.cat(emb_data_list, dim=1))\n",
        "\n",
        "  def number_of_output_neurons(self):\n",
        "    return self.total_num_out"
      ],
      "metadata": {
        "id": "nMHnUQkZmVWZ",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:44:55.221247Z",
          "iopub.execute_input": "2024-07-03T20:44:55.221664Z",
          "iopub.status.idle": "2024-07-03T20:44:55.231709Z",
          "shell.execute_reply.started": "2024-07-03T20:44:55.221631Z",
          "shell.execute_reply": "2024-07-03T20:44:55.230783Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Dataset and Data Module"
      ],
      "metadata": {
        "id": "-vV4u3kaNXRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "class SalesDataset(Dataset):\n",
        "  def __init__(self, tensor_seq: torch.Tensor, tensor_hist: torch.Tensor,\n",
        "               tensors_emb: list[torch.Tensor], tensor_aux: torch.Tensor,\n",
        "               df: pd.DataFrame):\n",
        "    self.tensor_seq = tensor_seq\n",
        "    self.tensor_hist = tensor_hist\n",
        "    self.tensors_emb = tensors_emb\n",
        "    self.tensor_aux = tensor_aux\n",
        "    self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.tensor_seq.shape[0]\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    seq = self.tensor_seq[i]\n",
        "    hist = self.tensor_hist[i]\n",
        "    emb = [tensor[i] for tensor in self.tensors_emb]\n",
        "    aux = self.tensor_aux[i]\n",
        "    return seq, hist, emb, aux\n",
        "\n",
        "class SalesTrainingDataset(SalesDataset):\n",
        "  def __init__(self, tensor_seq: torch.Tensor, tensor_hist: torch.Tensor,\n",
        "               tensors_emb: list[torch.Tensor], tensor_aux: torch.Tensor,\n",
        "               target: torch.Tensor, df: pd.DataFrame):\n",
        "    super().__init__(tensor_seq, tensor_hist, tensors_emb, tensor_aux, df)\n",
        "    self.target = target\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    batch = super().__getitem__(i)\n",
        "    target = self.target[i]\n",
        "    return *batch, target\n",
        "\n",
        "  def last_n(self, n: int):\n",
        "    seq = self.tensor_seq[-n:].contiguous()\n",
        "    hist = self.tensor_hist[-n:].contiguous()\n",
        "    emb = [t[-n:].contiguous() for t in self.tensors_emb]\n",
        "    aux = self.tensor_aux[-n:].contiguous()\n",
        "    target = self.target[-n:].contiguous()\n",
        "    df = self.df[-n:]\n",
        "    return SalesTrainingDataset(seq, hist, emb, aux, target, df)\n",
        "\n",
        "feat_fourier = [feat for feat in fourier_columns if feat not in [\n",
        "    'sin_M_1'\n",
        "]]\n",
        "feat_date=['year']\n",
        "feat_bool=['is_post_payment_date', 'is_free']\n",
        "feat_store_aux = ['population_city', 'population_state']\n",
        "feat_onehot = ['store_type']\n",
        "feat_numeric = ['store_size', 'cluster', 'oil_price']\n",
        "# Embedding dimensions\n",
        "feat_store_emb = ['store_nbr', 'store_type',\n",
        "                  'city', 'state']\n",
        "feat_store_emb_out = [32, 4, 8, 4] # dims in embedding space\n",
        "feat_store_emb_in = [len(col.unique()) for col in [\n",
        "    df_store.store_nbr, df_store.store_type,\n",
        "    df_store.city, df_store.state]]"
      ],
      "metadata": {
        "id": "QchjBBv0I07w",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:04:45.897554Z",
          "iopub.execute_input": "2024-07-03T21:04:45.898263Z",
          "iopub.status.idle": "2024-07-03T21:04:45.915348Z",
          "shell.execute_reply.started": "2024-07-03T21:04:45.898232Z",
          "shell.execute_reply": "2024-07-03T21:04:45.914326Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SalesDataModule(pl.LightningDataModule):\n",
        "  def __init__(self, hist_len: int, batch_size: int, num_val_days: int,\n",
        "               preprocessor: DataPipeline, feat_seq: list[str],\n",
        "               df_train=df_train, df_pred=df_test,\n",
        "               feat_emb=feat_store_emb, feat_aux=feat_store_aux,\n",
        "               target_columns=sales_columns, device=None, dtype=torch.float32):\n",
        "    super().__init__()\n",
        "    self.df_train = df_train\n",
        "    self.df_pred = df_pred\n",
        "    self.num_stores = len(df_pred.store_nbr.unique())\n",
        "    self.target_columns = target_columns\n",
        "    self.feat_seq = feat_seq\n",
        "    self.hist_len = hist_len\n",
        "    self.feat_emb = feat_emb\n",
        "    self.feat_aux = feat_aux\n",
        "    self.batch_size = batch_size\n",
        "    self.num_val_days = num_val_days\n",
        "    self.train = None\n",
        "    self.pred = None\n",
        "    self.num_days_pred = None\n",
        "    self.device = device\n",
        "    self.dtype = dtype\n",
        "    self.preprocessor = preprocessor\n",
        "\n",
        "  def setup(self, stage: str):\n",
        "    if stage == 'fit':\n",
        "      self.train, self.pred = self._setup_validation()\n",
        "    elif stage == 'predict':\n",
        "      self.train, self.pred = self._setup_prediction()\n",
        "\n",
        "  def _setup_validation(self):\n",
        "    val_split_date = self.df_train['date'].max() - pd.DateOffset(days=self.num_val_days-1)\n",
        "    df_train, df_val = split_by_date(self.df_train,\n",
        "      year=val_split_date.year, month=val_split_date.month, day=val_split_date.day)\n",
        "    df_train = self.preprocessor.fit_transform(df_train)\n",
        "    df_val = self.preprocessor.transform(df_val, include_sales=True)\n",
        "    self.num_days_pred = len(df_val.date.unique())\n",
        "    train = self._prepare_training_tensors(df_train)\n",
        "    val = self._prepare_prediction_tensors(df_train, df_val)\n",
        "    target = self._prepare_target_tensor(df_val)\n",
        "    return SalesTrainingDataset(*train, df_train), SalesTrainingDataset(*val, target, df_val)\n",
        "\n",
        "  def _setup_prediction(self):\n",
        "    df_train = self.preprocessor.fit_transform(self.df_train)\n",
        "    df_pred = self.preprocessor.transform(self.df_pred, include_sales=False)\n",
        "    self.num_days_pred = len(df_pred.date.unique())\n",
        "    train = self._prepare_training_tensors(df_train)\n",
        "    pred = self._prepare_prediction_tensors(df_train, df_pred)\n",
        "    return SalesTrainingDataset(*train, df_train), SalesDataset(*pred, df_pred)\n",
        "\n",
        "  def _prepare_tensor(self, data, dtype=None):\n",
        "    if dtype is None:\n",
        "      dtype = self.dtype\n",
        "    return torch.tensor(data, device=self.device, dtype=dtype)\n",
        "\n",
        "  def _prepare_target_tensor(self, df):\n",
        "    return self._prepare_tensor(df[self.target_columns].values)\n",
        "\n",
        "  def prepare_emb_aux_tensors(self, df):\n",
        "    emb = [self._prepare_tensor(df[feat].values, torch.int) for feat in self.feat_emb]\n",
        "    aux = self._prepare_tensor(df[self.feat_aux].values)\n",
        "    return emb, aux\n",
        "\n",
        "  def _prepare_training_tensors(self, df):\n",
        "    # hist is a tensor of shape\n",
        "    # (num_samples, hist_len+1, len(feat_seq + target_columns))\n",
        "    hist = self._create_windows(df, self.feat_seq + self.target_columns,\n",
        "                                remove_first_window=False)\n",
        "    hist, seq = torch.tensor_split(hist, (-1, ), dim=1)\n",
        "    seq = seq[:, 0, :] # last row is the actual feature and target data\n",
        "    seq, target = torch.tensor_split(seq, (len(self.feat_seq), ), dim=1)\n",
        "    emb, aux = self.prepare_emb_aux_tensors(df)\n",
        "    return seq, hist, emb, aux, target\n",
        "\n",
        "  def _prepare_prediction_tensors(self, df_train, df_pred):\n",
        "    hist = self._prepare_prediction_hist_tensor(df_train, df_pred)\n",
        "    seq = self._prepare_tensor(df_pred[self.feat_seq].values)\n",
        "    emb, aux = self.prepare_emb_aux_tensors(df_pred)\n",
        "    return seq, hist, emb, aux\n",
        "\n",
        "  def _prepare_prediction_hist_tensor(self, df_train, df_pred):\n",
        "    feat_hist = self.feat_seq + self.target_columns\n",
        "    num_feat_hist = len(feat_hist)\n",
        "    hist_len = self.hist_len\n",
        "    hist = np.zeros((len(df_pred), hist_len, num_feat_hist))\n",
        "    recent = np.zeros((self.num_stores, hist_len, num_feat_hist))\n",
        "    for store, store_data in df_train.groupby('store_nbr'):\n",
        "      recent_store = store_data.tail(hist_len)[feat_hist].values\n",
        "      pad = recent_store[0:1].repeat(repeats=hist_len-len(recent_store), axis=0)\n",
        "      recent[store, :, :]  = np.concatenate((pad, recent_store), axis=0)\n",
        "    for day in range(min(hist_len, self.num_days_pred)):\n",
        "      day_idx = day * self.num_stores\n",
        "      num_days_train = hist_len - day\n",
        "      hist[day_idx: (day_idx + self.num_stores), 0:num_days_train, :] = recent[\n",
        "          :,  -num_days_train:, :]\n",
        "    return self._prepare_tensor(hist)\n",
        "\n",
        "  def _sort_index_switch_store_date(self, df, days_to_remove):\n",
        "    # gets an index which resorts the order from (store_nbr, date) to (date, store_nbr)\n",
        "    df = df[['store_nbr', 'date']].sort_values(\n",
        "        by=['date', 'store_nbr']).reset_index(drop=True)\n",
        "    rows_per_store = []\n",
        "    for store, store_data in df.groupby('store_nbr'):\n",
        "      rows_per_store.append(store_data[days_to_remove:])\n",
        "    df = pd.concat(rows_per_store).reset_index(drop=True)\n",
        "    return df.sort_values(by=['date', 'store_nbr']).index\n",
        "\n",
        "  def _create_windows(self, df, columns, remove_first_window=False):\n",
        "    # Initialize the list to hold windowed data of shape (window_len, num_features)\n",
        "    windows, window_len = [], self.hist_len + 1\n",
        "    df = df.sort_values(by=['store_nbr', 'date'])\n",
        "    df = df.reset_index(drop=True)\n",
        "    stores = sorted(df['store_nbr'].unique())\n",
        "    start_index = window_len if remove_first_window else 0\n",
        "    sort_index = self._sort_index_switch_store_date(df,\n",
        "      days_to_remove=start_index)\n",
        "\n",
        "    for store in stores:\n",
        "      store_data = df[df['store_nbr'] == store]\n",
        "      store_data = store_data[columns].to_numpy()\n",
        "\n",
        "      # Create sliding windows for the store\n",
        "      for i in range(start_index, len(store_data)):\n",
        "        if remove_first_window or i >= window_len - 1:\n",
        "          start = i - window_len + 1\n",
        "          window = store_data[start:(i+1), :]\n",
        "        else:\n",
        "          pad = store_data[0:1].repeat(window_len - i - 1, 0)\n",
        "          window = store_data[0:(i+1), :]\n",
        "          window = np.vstack((pad, window))\n",
        "        windows.append(window)\n",
        "\n",
        "    # Convert to sorted PyTorch tensor\n",
        "    windows = np.array(windows).take(sort_index, axis=0)\n",
        "    return self._prepare_tensor(windows)\n",
        "\n",
        "  def get_transformed_train(self, num_days_past):\n",
        "    return self.train.df[-num_days_past*self.num_stores:]\n",
        "  def get_transformed_pred(self):\n",
        "    return self.pred.df\n",
        "  def inverse_transform(self, df, unseen: bool):\n",
        "    return self.preprocessor.inverse_transform_target(df, unseen=unseen)\n",
        "\n",
        "  def _dataset_loader(self, dataset, batch_size, num_workers=1):\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
        "                      num_workers=num_workers)\n",
        "  def train_dataloader(self):\n",
        "    return self._dataset_loader(self.train, batch_size=self.batch_size, num_workers=3)\n",
        "  def val_dataloader(self):\n",
        "    return self._dataset_loader(self.pred, batch_size=self.num_stores)\n",
        "  def train_pred_dataloader(self, days=28):\n",
        "    train = self.train.last_n(self.num_stores * days)\n",
        "    return self._dataset_loader(train, batch_size=self.num_stores)\n",
        "  def prediction_dataloader(self):\n",
        "    return self._dataset_loader(self.pred, batch_size=self.num_stores)\n",
        "  def add_autoregression(self, day: int, pred_day: torch.Tensor):\n",
        "    # pred_day.shape==(num_stores, hist_len, len(feat_seq+sales_columns))\n",
        "    hist = self.pred.tensor_hist\n",
        "    for fut in range(day + 1, self.num_days_pred):\n",
        "      if fut - day > self.hist_len:\n",
        "        break\n",
        "      fut_idx = fut * self.num_stores\n",
        "      next_fut_idx = fut_idx + self.num_stores\n",
        "      hist_idx = self.hist_len - (fut-day)\n",
        "      hist[fut_idx: next_fut_idx, hist_idx, :] = pred_day"
      ],
      "metadata": {
        "id": "42mn_9ISD-XG",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:04:55.155699Z",
          "iopub.execute_input": "2024-07-03T21:04:55.156066Z",
          "iopub.status.idle": "2024-07-03T21:04:55.194131Z",
          "shell.execute_reply.started": "2024-07-03T21:04:55.156037Z",
          "shell.execute_reply": "2024-07-03T21:04:55.193189Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = DataPipeline(\n",
        "  feat_minmax=feat_date+feat_bool,\n",
        "  feat_onehot=['store_type'],\n",
        "  feat_ordinal=['store_type', 'city', 'state'],\n",
        "  feat_std=feat_numeric+feat_store_aux,\n",
        "  feat_pass=feat_fourier+['date', 'store_nbr'],\n",
        "  feat_storewise=onpromotion_columns,\n",
        "  filter_closed_outlier=False,\n",
        "  apply_power=True,\n",
        "  apply_scaling=True,\n",
        "  diff_period=0,\n",
        "  lag_period=0\n",
        ")\n",
        "feat_onehot = ['store_type_' + c for c in 'BCDE'] # Drop type A\n",
        "feat_seq = feat_date + feat_fourier + feat_bool + feat_onehot + feat_numeric + onpromotion_columns + pipe.feat_lag\n",
        "start_day = pd.Timestamp(year=2016, month=1, day=1)\n",
        "model = None # clear up RAM and old reference to datamodule\n",
        "datamodule = SalesDataModule(hist_len=24*7, batch_size=2**13, # 2**13 optimal for 15 GB GPU from Colab\n",
        "                            num_val_days=31,\n",
        "                            preprocessor=pipe, feat_seq=feat_seq,\n",
        "                            df_train=df_train[df_train.date >= start_day]\n",
        "                            )\n",
        "datamodule.setup('fit')"
      ],
      "metadata": {
        "id": "06jUuV2uJiFc",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:05:34.920256Z",
          "iopub.execute_input": "2024-07-03T21:05:34.921189Z",
          "iopub.status.idle": "2024-07-03T21:05:56.441288Z",
          "shell.execute_reply.started": "2024-07-03T21:05:34.921136Z",
          "shell.execute_reply": "2024-07-03T21:05:56.440404Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Lightning Recurrent NN Module"
      ],
      "metadata": {
        "id": "fCuM5pk6scnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSLE: # root-mean-squared-log-error\n",
        "  @staticmethod\n",
        "  def loss(pred, actual):\n",
        "    pred = torch.log(torch.clip(pred, 1e-10, None) + 1.)\n",
        "    actual = torch.log(torch.clip(actual, 1e-10, None) + 1.)\n",
        "    return torch.abs(torch.sqrt(torch.mean(torch.square(pred - actual))))\n",
        "\n",
        "  def loss_df(pred, actual, target_columns=sales_columns):\n",
        "    pred =  torch.tensor(pred[target_columns].values)\n",
        "    actual = torch.tensor(actual[target_columns].values)\n",
        "    return RMSLE.loss(pred, actual)\n",
        "\n",
        "class SalesPredictionModel(pl.LightningModule):\n",
        "  def __init__(self, lr: float, datamodule: SalesDataModule, emb_in=feat_store_emb_in, emb_out=feat_store_emb_out, num_target=len(sales_columns),\n",
        "               num_feat_seq=len(feat_seq), num_feat_aux=len(feat_store_aux),\n",
        "               dense_hidden_size=256, dropout=0.25,\n",
        "               recurrent_num_layers=2, recurrent_past_out=[1], recurrent_hidden_size=len(sales_columns)):\n",
        "    super().__init__()\n",
        "    self.lr = lr\n",
        "    self.emb_layer = StoreEmbeddingLayer(emb_in, emb_out, num_feat_aux)\n",
        "    self.recurrent = nn.LSTM(\n",
        "        input_size=num_feat_seq + num_target,\n",
        "        hidden_size=recurrent_hidden_size,\n",
        "        batch_first=True, num_layers=recurrent_num_layers, dropout=dropout\n",
        "    )\n",
        "    self.recurrent_past_out = recurrent_past_out\n",
        "    self.h_0, self.c_0 = None, None # (num_layers, num_stores, hidden_size) saves state of last predicted day\n",
        "    num_emb_out = self.emb_layer.number_of_output_neurons()\n",
        "    # Final layers combine embeddings with recurrent layers and give positive sale\n",
        "    self.combined_dense = nn.Sequential(\n",
        "        nn.Linear(num_feat_seq + len(\n",
        "            recurrent_past_out) * recurrent_hidden_size + num_emb_out,\n",
        "                  dense_hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(dense_hidden_size),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(dense_hidden_size, num_target),\n",
        "        nn.Softplus()\n",
        "    )\n",
        "    self.datamodule = datamodule\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "     return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "  def forward(self, seq, hist, emb, aux, recurrent_hidden_state=None):\n",
        "    emb_aux = self.emb_layer(emb, aux)\n",
        "    seq_hist, (self.h_0, self.c_0) = self.recurrent(hist, recurrent_hidden_state) # contains historical timesteps\n",
        "    seq_hist = [seq_hist[:, -i, :] for i in self.recurrent_past_out] # get previous timesteps hidden representation\n",
        "    out = self.combined_dense(torch.cat((seq, *seq_hist, emb_aux), dim=1))\n",
        "    return out\n",
        "\n",
        "  def get_store_embedding(self, emb: list[torch.Tensor], aux: torch.Tensor):\n",
        "    return self.emb_layer(emb, aux)\n",
        "\n",
        "  def get_recurrent_hidden_state(self, num_states):\n",
        "    if self.h_0 is None:\n",
        "      return None\n",
        "    h_0 = self.h_0[:, -num_states:, :]\n",
        "    c_0 = self.c_0[:, -num_states:, :]\n",
        "    return (h_0.contiguous(), c_0.contiguous())\n",
        "  def reset_recurrent_hidden_state(self):\n",
        "    self.h_0 = None\n",
        "    self.c_0 = None\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    *batch, Y_actual = batch\n",
        "    Y_pred = self(*batch)\n",
        "    loss = RMSLE.loss(Y_pred, Y_actual)\n",
        "    self.log('train_loss', loss, prog_bar=True)\n",
        "    return loss\n",
        "\n",
        "  def step_with_autoregression(self, batch, batch_idx):\n",
        "    seq, hist, emb, aux  = batch[0], batch[1], batch[2], batch[3]\n",
        "    Y_pred = self(seq, hist, emb, aux,\n",
        "                  self.get_recurrent_hidden_state(seq.shape[0]))\n",
        "    seq_pred = torch.concat((seq, Y_pred), dim=1)\n",
        "    self.datamodule.add_autoregression(day=batch_idx, pred_day=seq_pred)\n",
        "    return Y_pred\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "   Y_pred = self.step_with_autoregression(batch, batch_idx)\n",
        "   Y_actual = batch[4]\n",
        "   loss = RMSLE.loss(Y_pred, Y_actual)\n",
        "   self.log('val_loss', loss, prog_bar=True)\n",
        "   return loss\n",
        "\n",
        "  def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "    return self.step_with_autoregression(batch, batch_idx)\n",
        "\n",
        "model = SalesPredictionModel(0.001, datamodule)\n",
        "print(model)\n",
        "print(f'Number of Parameters: {sum(p.numel() for p in model.parameters())}')"
      ],
      "metadata": {
        "id": "cd6gZtkN4YbA",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:05:56.44358Z",
          "iopub.execute_input": "2024-07-03T21:05:56.444154Z",
          "iopub.status.idle": "2024-07-03T21:05:56.472789Z",
          "shell.execute_reply.started": "2024-07-03T21:05:56.444112Z",
          "shell.execute_reply": "2024-07-03T21:05:56.471744Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Prediction"
      ],
      "metadata": {
        "id": "2Y4bQaGLkqh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find hyperparams with Validation Dataset"
      ],
      "metadata": {
        "id": "y2JkabuossQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import EarlyStopping, StochasticWeightAveraging\n",
        "logger = CSVLogger('logs', name='experiment', version=0)\n",
        "\n",
        "learning_rate = 0.005\n",
        "recurrent_num_layers = 3\n",
        "recurrent_past_out = [1,7,14,21,28]\n",
        "recurrent_hidden_size = len(sales_columns)\n",
        "dense_hidden_size = 256\n",
        "dropout = 0.25\n",
        "model = SalesPredictionModel(datamodule=datamodule, lr=learning_rate,\n",
        "                             recurrent_num_layers=recurrent_num_layers,\n",
        "                             recurrent_past_out=recurrent_past_out,\n",
        "                             recurrent_hidden_size=recurrent_hidden_size,\n",
        "                             dense_hidden_size=dense_hidden_size,\n",
        "                             dropout=dropout)\n",
        "epochs = 30\n",
        "\n",
        "accumulate_grad_batches = 1\n",
        "gradient_clip_val = 10.0\n",
        "\n",
        "early_stop_cb = EarlyStopping(\n",
        "    monitor='val_loss', mode='min',\n",
        "    patience=6, min_delta=0.0001,\n",
        "    verbose=False,\n",
        ")\n",
        "swa_lrs = 1e-2\n",
        "swa_cb = StochasticWeightAveraging(swa_lrs=swa_lrs)\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=epochs, callbacks=[swa_cb, early_stop_cb], deterministic='warn',\n",
        "                     gradient_clip_val=gradient_clip_val, accumulate_grad_batches=accumulate_grad_batches,\n",
        "                     default_root_dir=KAGGLE_WORKING_PATH, logger=logger, log_every_n_steps=1)\n",
        "trainer.fit(model, train_dataloaders=datamodule.train_dataloader(),\n",
        "            val_dataloaders=datamodule.val_dataloader())\n",
        "torch.cuda.empty_cache() # avoids out of memory if trained again..."
      ],
      "metadata": {
        "id": "GBvF4CWVhsoL",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:06:31.246791Z",
          "iopub.execute_input": "2024-07-03T21:06:31.24719Z",
          "iopub.status.idle": "2024-07-03T21:09:19.359989Z",
          "shell.execute_reply.started": "2024-07-03T21:06:31.24716Z",
          "shell.execute_reply": "2024-07-03T21:09:19.358931Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pl_logs(root_path=KAGGLE_WORKING_PATH,\n",
        "                 project_name='Lightning',\n",
        "                 version=0):\n",
        "  log_path = os.path.join(root_path, f'logs/{project_name}/version_{version}/metrics.csv')\n",
        "  return pd.read_csv(log_path)\n",
        "\n",
        "def plot_metrics(metrics: pd.DataFrame, figsize=(20,10), x='epoch'):\n",
        "  prog_columns = ['epoch', 'step']\n",
        "  loss_columns = [col for col in list(metrics.columns) if 'loss' in col]\n",
        "  plt.figure(figsize=figsize)\n",
        "  for col in loss_columns:\n",
        "    df = metrics[prog_columns + [col]].dropna()\n",
        "    sns.lineplot(data=df, x=x, y=col, label=col)\n",
        "  plt.xlabel(x)\n",
        "  plt.ylabel('RMSLE')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "plot_metrics(load_pl_logs(project_name='experiment'), x='epoch')"
      ],
      "metadata": {
        "id": "NMJBJTYChIVG",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:09:19.362037Z",
          "iopub.execute_input": "2024-07-03T21:09:19.362403Z",
          "iopub.status.idle": "2024-07-03T21:09:20.78388Z",
          "shell.execute_reply.started": "2024-07-03T21:09:19.362339Z",
          "shell.execute_reply": "2024-07-03T21:09:20.78291Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inverse_transformed_fit(model, trainer, datamodule, num_days_past=4*7, target_columns=sales_columns):\n",
        "  include_columns = ['date', 'store_nbr']\n",
        "  train = datamodule.get_transformed_train(num_days_past)\n",
        "  val = datamodule.get_transformed_pred()\n",
        "  train_pred, val_pred = train[include_columns].copy(), val[include_columns].copy()\n",
        "  model.reset_recurrent_hidden_state()\n",
        "  train_pred[target_columns] = np.concatenate(trainer.predict(\n",
        "      model, dataloaders=datamodule.train_pred_dataloader(num_days_past)), axis=0)\n",
        "  val_pred[target_columns] = np.concatenate(trainer.predict(\n",
        "      model, dataloaders=datamodule.prediction_dataloader()), axis=0)\n",
        "  print('Training Loss before inversion: ', RMSLE.loss_df(train_pred, train))\n",
        "  print('Validation Loss before inversion: ', RMSLE.loss_df(val_pred, val))\n",
        "  train = datamodule.inverse_transform(train, unseen=False)\n",
        "  train_pred = datamodule.inverse_transform(train_pred, unseen=False)\n",
        "  val = datamodule.inverse_transform(val, unseen=True)\n",
        "  val_pred = datamodule.inverse_transform(val_pred, unseen=True)\n",
        "  print('Training Loss AFTER inversion: ', RMSLE.loss_df(train_pred, train))\n",
        "  print('Validation Loss AFTER inversion: ', RMSLE.loss_df(val_pred, val))\n",
        "  return dict(train=train, val=val, train_pred=train_pred, val_pred=val_pred)\n",
        "\n",
        "actual_pred = get_inverse_transformed_fit(model, trainer, datamodule, num_days_past=12*7)\n",
        "# val AFTER 0.4482 (31 val days, 01.2016, 24*7 hist, no filtering (and no flags), 40 epochs)\n",
        "df = actual_pred['val_pred']\n",
        "df[(df[sales_columns]<0).any(axis=1)]"
      ],
      "metadata": {
        "id": "pUyDG1Ugvz42",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:09:32.92673Z",
          "iopub.execute_input": "2024-07-03T21:09:32.92714Z",
          "iopub.status.idle": "2024-07-03T21:09:40.949726Z",
          "shell.execute_reply.started": "2024-07-03T21:09:32.927106Z",
          "shell.execute_reply": "2024-07-03T21:09:40.948602Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_prediction(actual_pred: dict,\n",
        "                    families=ana_families,\n",
        "                    stores=ana_stores):\n",
        "  nrows, ncols = len(stores), len(families)\n",
        "  fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 20))\n",
        "  for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "      store, family = stores[i], families[j]\n",
        "      filter = lambda df: df[(df.store_nbr == store)]\n",
        "      ax_xy = ax[i,j]\n",
        "      for label, df in actual_pred.items():\n",
        "        sns.lineplot(x='date', y=family, data=filter(df), label=label, ax=ax_xy)\n",
        "      ax_xy.set_title(f'Store {store} - Product {family}')\n",
        "  plt.tight_layout()\n",
        "show_prediction(actual_pred)"
      ],
      "metadata": {
        "id": "uxrIFxEfwTKc",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:10:05.658011Z",
          "iopub.execute_input": "2024-07-03T21:10:05.658818Z",
          "iopub.status.idle": "2024-07-03T21:10:18.843301Z",
          "shell.execute_reply.started": "2024-07-03T21:10:05.658779Z",
          "shell.execute_reply": "2024-07-03T21:10:18.842045Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Final Model on all data and forecast"
      ],
      "metadata": {
        "id": "rtmySft9grQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger = CSVLogger('logs', name='prediction', version=0)\n",
        "datamodule.setup('predict')\n",
        "model = SalesPredictionModel(datamodule=datamodule, lr=learning_rate,\n",
        "                             recurrent_num_layers=recurrent_num_layers,\n",
        "                             recurrent_past_out=recurrent_past_out,\n",
        "                             recurrent_hidden_size=recurrent_hidden_size,\n",
        "                             dense_hidden_size=dense_hidden_size,\n",
        "                             dropout=dropout)\n",
        "swa_cb = StochasticWeightAveraging(swa_lrs=swa_lrs)\n",
        "trainer = pl.Trainer(max_epochs=30, callbacks=[swa_cb],\n",
        "                     gradient_clip_val=gradient_clip_val, accumulate_grad_batches=accumulate_grad_batches,\n",
        "                     default_root_dir=KAGGLE_WORKING_PATH, logger=logger, log_every_n_steps=1)\n",
        "trainer.fit(model, train_dataloaders=datamodule.train_dataloader())"
      ],
      "metadata": {
        "id": "F84EEUyOfkw5",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:11:10.067765Z",
          "iopub.execute_input": "2024-07-03T21:11:10.068661Z",
          "iopub.status.idle": "2024-07-03T21:13:59.168471Z",
          "shell.execute_reply.started": "2024-07-03T21:11:10.068626Z",
          "shell.execute_reply": "2024-07-03T21:13:59.167493Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(load_pl_logs(project_name='prediction'), x='epoch')"
      ],
      "metadata": {
        "id": "pYXP2AO9rbJV",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:13:59.171096Z",
          "iopub.execute_input": "2024-07-03T21:13:59.171668Z",
          "iopub.status.idle": "2024-07-03T21:14:00.484956Z",
          "shell.execute_reply.started": "2024-07-03T21:13:59.171623Z",
          "shell.execute_reply": "2024-07-03T21:14:00.484055Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inverse_transformed_forecast(model, trainer, datamodule, num_days_past=4*7,\n",
        "                                     target_columns=sales_columns,\n",
        "                                     id_columns=id_columns):\n",
        "  include_columns = ['date', 'store_nbr']\n",
        "  train = datamodule.get_transformed_train(num_days_past)\n",
        "  train_pred = train[include_columns].copy()\n",
        "  forecast = datamodule.get_transformed_pred()[\n",
        "      include_columns + id_columns].copy()\n",
        "  model.reset_recurrent_hidden_state()\n",
        "  train_pred[target_columns] = np.concatenate(trainer.predict(\n",
        "      model, dataloaders=datamodule.train_pred_dataloader(num_days_past)), axis=0)\n",
        "  forecast[target_columns] = np.concatenate(trainer.predict(\n",
        "      model, dataloaders=datamodule.prediction_dataloader()), axis=0)\n",
        "  print('Training Loss before inversion: ', RMSLE.loss_df(train_pred, train))\n",
        "  train = datamodule.inverse_transform(train, unseen=False)\n",
        "  train_pred = datamodule.inverse_transform(train_pred, unseen=False)\n",
        "  forecast = datamodule.inverse_transform(forecast, unseen=True)\n",
        "  print('Training Loss AFTER inversion: ', RMSLE.loss_df(train_pred, train))\n",
        "  return dict(train=train, train_pred=train_pred, forecast=forecast)\n",
        "\n",
        "forecast = get_inverse_transformed_forecast(model, trainer, datamodule,\n",
        "                                            num_days_past=24*7)\n",
        "df = forecast['forecast']\n",
        "df[(df[sales_columns]<0).any(axis=1)]"
      ],
      "metadata": {
        "id": "gEmegI8y5ki0",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:14:10.964136Z",
          "iopub.execute_input": "2024-07-03T21:14:10.965636Z",
          "iopub.status.idle": "2024-07-03T21:14:19.110824Z",
          "shell.execute_reply.started": "2024-07-03T21:14:10.965586Z",
          "shell.execute_reply": "2024-07-03T21:14:19.109858Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_prediction(forecast)"
      ],
      "metadata": {
        "id": "0RD_v9gcLwud",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:14:25.168456Z",
          "iopub.execute_input": "2024-07-03T21:14:25.168826Z",
          "iopub.status.idle": "2024-07-03T21:14:34.965324Z",
          "shell.execute_reply.started": "2024-07-03T21:14:25.168795Z",
          "shell.execute_reply": "2024-07-03T21:14:34.964423Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = forecast['forecast']\n",
        "submission = pd.DataFrame(data=dict(id=pd.concat([df[col] for col in id_columns]),\n",
        "             sales=pd.concat([df[col] for col in sales_columns]))).sort_values(\n",
        "                 by='id')\n",
        "submission.to_csv(os.path.join(\n",
        "    KAGGLE_WORKING_PATH, 'submission_torch.csv'), index=False)\n",
        "submission"
      ],
      "metadata": {
        "id": "of_vYyI-dp_H",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:15:24.307059Z",
          "iopub.execute_input": "2024-07-03T21:15:24.30773Z",
          "iopub.status.idle": "2024-07-03T21:15:24.410127Z",
          "shell.execute_reply.started": "2024-07-03T21:15:24.307697Z",
          "shell.execute_reply": "2024-07-03T21:15:24.409224Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Final Store Embeddings"
      ],
      "metadata": {
        "id": "J4tbNeOLmeNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_store_embeddings(model, datamodule, feat_emb=feat_store_emb, feat_aux=feat_store_aux):\n",
        "  df = datamodule.get_transformed_pred()\n",
        "  cols = feat_emb + feat_aux\n",
        "  df = df[cols].drop_duplicates()\n",
        "  emb, aux = datamodule.prepare_emb_aux_tensors(df)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    store_emb = model.get_store_embedding(emb, aux)\n",
        "  store_emb_columns = [f'store_emb_{i}' for i in range(store_emb.shape[-1])]\n",
        "  df[store_emb_columns] = store_emb.cpu().numpy()\n",
        "  return df\n",
        "\n",
        "df_store_emb = get_store_embeddings(model, datamodule)\n",
        "df_store_emb.to_csv(os.path.join(KAGGLE_WORKING_PATH, 'store_embedding.csv'), index=False)\n",
        "df_store_emb"
      ],
      "metadata": {
        "id": "-dFw3mQBmkjC",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:15:29.968243Z",
          "iopub.execute_input": "2024-07-03T21:15:29.969095Z",
          "iopub.status.idle": "2024-07-03T21:15:30.070865Z",
          "shell.execute_reply.started": "2024-07-03T21:15:29.969058Z",
          "shell.execute_reply": "2024-07-03T21:15:30.069925Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "# show_dimensionality_reduced_clustering\n",
        "def analyze_clustering(df, df_store_emb, cluster_columns=[\n",
        "    'cluster', 'store_size_group', 'store_type']):\n",
        "  df = SalesDataExtractor().merge_source_data(df)[\n",
        "    ['store_nbr'] + cluster_columns].drop_duplicates().sort_values(by='store_nbr')\n",
        "  df_store_emb = df_store_emb.sort_values(by='store_nbr')\n",
        "  store_emb_columns = [col for col in list(df_store_emb.columns) if col.startswith('store_emb')]\n",
        "  embeddings = df_store_emb[store_emb_columns].values\n",
        "  sil_score = [silhouette_score(embeddings, df[col].values) for col in cluster_columns]\n",
        "  db_index = [davies_bouldin_score(embeddings, df[col].values) for col in cluster_columns]\n",
        "  # Visualize 2D Dimensionality reduction using Principal-Components\n",
        "  # t-distributed stochastic neighbor embedding\n",
        "  embeddings_pca = PCA(n_components=2).fit_transform(embeddings)\n",
        "  embeddings_tsne = TSNE(n_components=2, random_state=SEED).fit_transform(embeddings)\n",
        "  nrows = len(cluster_columns)\n",
        "  fig, ax = plt.subplots(ncols=2, nrows=nrows, figsize=(20, 4*nrows))\n",
        "  for i, col in enumerate(cluster_columns):\n",
        "    clusters = df[col].values\n",
        "    ax_i = ax[i,0]\n",
        "    sns.scatterplot(x=embeddings_pca[:, 0], y=embeddings_pca[:, 1],\n",
        "                  hue=clusters, palette='husl', legend='full', ax=ax_i)\n",
        "    ax_i.set_title(f'PCA of Store Embeddings for {col}')\n",
        "    ax_i.set_xlabel('PCA Component 1')\n",
        "    ax_i.set_ylabel('PCA Component 2')\n",
        "    ax_i = ax[i,1]\n",
        "    sns.scatterplot(x=embeddings_tsne[:, 0], y=embeddings_tsne[:, 1],\n",
        "                  hue=clusters, palette='husl', legend='full', ax=ax_i)\n",
        "    ax_i.set_title(f't-SNE of Store Embeddings for {col}')\n",
        "    ax_i.set_xlabel('t-SNE Component 1')\n",
        "    ax_i.set_ylabel('t-SNE Component 2')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  return pd.DataFrame(index=cluster_columns,\n",
        "                      data=dict(sil_score=sil_score, db_index=db_index))\n",
        "analyze_clustering(df_test, df_store_emb)"
      ],
      "metadata": {
        "id": "H50tMpLgG070",
        "execution": {
          "iopub.status.busy": "2024-07-03T21:15:37.485178Z",
          "iopub.execute_input": "2024-07-03T21:15:37.485912Z",
          "iopub.status.idle": "2024-07-03T21:15:41.521564Z",
          "shell.execute_reply.started": "2024-07-03T21:15:37.485873Z",
          "shell.execute_reply": "2024-07-03T21:15:41.520624Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -la"
      ],
      "metadata": {
        "id": "VVcWWomPhMcm",
        "execution": {
          "iopub.status.busy": "2024-07-03T20:59:15.546548Z",
          "iopub.execute_input": "2024-07-03T20:59:15.54693Z",
          "iopub.status.idle": "2024-07-03T20:59:16.756944Z",
          "shell.execute_reply.started": "2024-07-03T20:59:15.5469Z",
          "shell.execute_reply": "2024-07-03T20:59:16.755589Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}